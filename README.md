"Хотите удивительную историю? Маленького Сашу отец часто посылал в магазин за мандаринами - давал ему пару сотен рублей, а маленький Саша возвращался с мандаринами и сдачей. Как-то раз отец что-то перепутал, и вместо ста рублей выдал сто индонезийских рупий. Ну всякое бывает, спросите вы, что ж тут удивительного. А вот что: маленький Саша сразу же понял, что тут что-то не так!!!"

13.02.17 - 18.02.17
Интересная постановка задачи - для обучения дана выборка из некоторого распределения, затем подаются новые точки из него же и какие-то левые, нужно отделять первые от вторых.

[Статья по Isolation Forest](http://cs.nju.edu.cn/zhouzh/zhouzh.files/publication/icdm08b.pdf)
Краткий перессказ статьи - Isolation Forest круче всех. Работая в предположении "аномальные точки - изолированные, то есть с низкой плотностью априорного распределения", можно за линейное время (с константой, не пропорциональной размерности пространства) научиться выдавать объектам адекватный скор аномальности, превосходящей по метрике AUC алгоритмы-предшественники (причём, якобы, особенно в многомерном случае - почему так, мне непонятно).

Ознакомился с Isolation Forest (Isolation Forest First Look.ipynb) и его sklearn-овской реализацией. Появилось страстное желание сделать для каждого дерева случайный выбор базиса (позже: это называется умными словами rotated bagging). Проведены эксперименты для разного числа деревьев в следующих случаях: два кластера; один кластер; два кластера с аномалиями в обучении; кластер в форме синуса. Во всех случаях контуры одинакового скора аномальности становились эстетически красивее. По мере auc-roc эти искусственные задачи Isolation Forest решает с высоким результатом и за линейное время, однако выбор адекватной метрики оценивания качества работы детектора аномалий - нетривиальный вопрос.

Существенные вопросы:
- зависимость качества работы алгоритма от размера случайных подвыборок для каждого дерева
- как влияет выбор случайного базиса на качество работы алгоритма (и почему все спокойно живут с этими полосами)
- выбор оптимального порога скора аномальности для отделения аномалий от нормалий... о, отличное слово, от нормалий, значит, на тестовой выборке.
- доп. вопрос: как связана задача с её решением в одномерном случае независимо для каждого признака?

__________________________________
Видимо, в дальнейшем по дефолту заметки будут делаться по мере прочитывания мной книги Aggarwal, Charu C. (не берусь транлитерировать эту фамилию) "Outlier Analysis". Если там буду какие-то страницы или разделы указывать без пояснений, то это, значит, оттуда.

21.02.17
О постановке задачи. Другим определением аномалии может быть "Есть некоторая нормальная модель, описывающая исходная данные. Аномалия - это то, что плохо вписывается в эту модель". Встречаются случаи, когда все аномалии похожи друг на друга и скапливаются в одной точке. В этом случае предположение, например, о том, что у аномалий относительно далеко находятся ближайшие соседи может не зайти. Также: исходя из такого определения, одним из теоретически оптимальных алгоритмов является применить какой-нибудь моделирующий алгоритм машинного обучения на всех данных в целом и выделить в качестве аномалий объекты с наибольшим отступом.

Также помимо аномалий существует такое понятие, как "шум". Отделять одно от другого задача довольно таки гробовая, однако природа этих вещей разная => и подход к ним тоже может оказаться разный.

25.02.17
Пара вчерашних соображений о кратчайших путях (р. 1.5). Допустим, есть данные специфического характера (временной ряд, последовательность символов, т.д.). Согласно общим рассуждениям, решать задачу можно так: 1) построить признаковое пространство по этим данным 2) решить задачу для этого пространства. Естественно, что могут найтись шорткаты - например, в последовательностях символов вдруг обнаружился совершенно новый неизвестный науке иероглиф. Однако глубокого методического смысла в подробном изучение шорткатов я не вижу - это всегда справедливо, что наличие дополнительной информации для общей задачи влечёт некоторую вероятность существования упрощения общего алгоритма. В силу этого, интересно рассматривать только общие задачи (с обучением на неразмеченных данных; с обучением на нормалиях; возможно, с обучением на аномалиях, хотя у меня не получилось это представить, ведь раз есть данные только об аномалиях, значит их, очевидно, физически больше чем нормалий, и надо просто инвертировать задачу; с обучением на размеченных данных).

Ещё хотел сделать метку: в силу того, что вся теория о bias-variance разложении верна и для обучения без учителя, все стандартные утверждения об ансамблировании справедливы и для задачи обнаружения аномалий. Это, например, даёт некоторое теоретическое обоснование, почему работает тот же Isolation Forest.

25.02.17
Сразу по нескольким причинам заинтересовался решением этой задачи в одномерном случае. Из оригинальных причин - допустим, в задаче с общей постановкой некоторым образом был получен anomaly_score (эту штуку надо тоже как-то ёмко называть... "степень аномальности" слишком длинно...), однако разделение на аномалии и нормалии неизвестно. Тогда, можно сказать, просто задача сведена к одномерному случаю. На прямой даны точки, нужно выделить аномалии. Судя по опыту Isolation Forest, самым простым случаем является тот, в котором степень аномальности нормалий сжимается в кластеры ([см. как располагаются скоры аномальности на правых картинках при большом числе деревьев](http://nbviewer.jupyter.org/github/FortsAndMills/ThinkAnomalouslyToFindAnomalies/blob/master/Isolation%20Forest%20First%20Look.ipynb)). То есть если человеческий глаз посмотрит на такую картинку, он сразу выделит "пустые пространства" вокруг этих кластеров и назовёт очень похожий на правду ответ.

Рассуждения о том, как можно решать задачу "на прямой дан набор точек, выделить аномалии" решил вынести в ноутбук.

26.02.17
Ноутбук [выложил](http://nbviewer.jupyter.org/github/FortsAndMills/ThinkAnomalouslyToFindAnomalies/blob/master/1D%20Anomaly%20Detection.ipynb). При прочтении желательно находиться подальше от автора и тяжёлых предметов, в конце концов, у меня были праздники.

Конечно, наверняка есть более классические способы решать задачу. Например, можно как-то узнать, что вот нормалии распределены по нормальному закону, вот примерный центр, дисперсия, правила трёх сигм... Но вот по какому порогу правдоподобия отделять - неясно, три сигмы это просто число хорошее, как десять или шестьдесят шесть. А ведь интуитивно кажется, что это как раз и есть самое интересное! И, конечно, так получилось, что в ноутбуке мне было интересно поработать в режиме минимальной доп. информации вроде количества кластеров или, тем более, предположений о распределении данных.
